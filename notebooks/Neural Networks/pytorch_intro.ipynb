{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PyTorch quick start guide\n",
   "id": "dddceb13ae40875f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T10:52:55.024565Z",
     "start_time": "2025-01-28T10:52:52.704787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Device configuration\n",
    "device = 'cpu'"
   ],
   "id": "660e595912cfa32a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "We can start with MNIST example:"
   ],
   "id": "5893c084238f8007"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-28T10:52:55.052651Z",
     "start_time": "2025-01-28T10:52:55.027570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Transform for preprocessing the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can then define dataset loaders:",
   "id": "d855b84e215ad47f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T10:52:55.113461Z",
     "start_time": "2025-01-28T10:52:55.111112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
   ],
   "id": "1f5d3ef970cec6d0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Networks are defined similar to keras:",
   "id": "efbb597ab9c329c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T10:52:55.122599Z",
     "start_time": "2025-01-28T10:52:55.119573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # Flatten the image\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x"
   ],
   "id": "23e4b9ef599aaedf",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can then create the model and optimizer:",
   "id": "2c17aaed80c83878"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T10:52:55.130801Z",
     "start_time": "2025-01-28T10:52:55.127553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = NeuralNet().to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "e819ff15a55ef9b7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In contrast with keras, we need to write our training function:",
   "id": "c38c68dea09eac76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T10:52:55.138777Z",
     "start_time": "2025-01-28T10:52:55.135862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, train_loader, optimizer, criterion, num_epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            images, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            if len(labels.size())>1:\n",
    "                labels = labels.squeeze(-1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n"
   ],
   "id": "b0ac54cb08610593",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As well as evaluation function:",
   "id": "6aa61ff7cb11f4ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T10:52:55.146562Z",
     "start_time": "2025-01-28T10:52:55.143787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            if len(labels.size())>1:\n",
    "                labels = labels.squeeze(-1)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy: {100 * correct / total:.2f}%')"
   ],
   "id": "aa5f422de58282a7",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then, we are ready to train and evaluate our model:",
   "id": "2141c80b867459b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T10:53:25.578813Z",
     "start_time": "2025-01-28T10:52:55.150310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the model:\n",
    "train_model(model, train_loader, optimizer, criterion, num_epochs=5)\n"
   ],
   "id": "7fb5511da9b34455",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.3970\n",
      "Epoch [2/5], Loss: 0.1926\n",
      "Epoch [3/5], Loss: 0.1428\n",
      "Epoch [4/5], Loss: 0.1148\n",
      "Epoch [5/5], Loss: 0.0961\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T10:53:26.498706Z",
     "start_time": "2025-01-28T10:53:25.584321Z"
    }
   },
   "cell_type": "code",
   "source": "evaluate_model(model, test_loader)",
   "id": "7cd9a0ebe8e0d919",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.84%\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can easily do the same with a Chelo dataset!!",
   "id": "1ec6e7f094f9da0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T10:53:27.083897Z",
     "start_time": "2025-01-28T10:53:26.504214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from chelo import DatasetRegistry\n",
    "\n",
    "dataset = DatasetRegistry.get_dataset(\"AmesMutagenicityDataset\")\n",
    "dataset.load_data()\n",
    "dataset_torch = dataset.to_pytorch()\n",
    "train_set, val_set = torch.utils.data.random_split(dataset_torch, [5000, 764])\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=val_set, batch_size=64, shuffle=False)\n"
   ],
   "id": "75af23506fe4517c",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We similarly define the network:",
   "id": "45dd44ba231345a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T10:53:27.092816Z",
     "start_time": "2025-01-28T10:53:27.089313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(8, 512)\n",
    "        self.fc2 = nn.Linear( 512, 2)\n",
    "        self.act = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.softmax(self.fc2(x))\n",
    "        return x"
   ],
   "id": "5ec6f0b0a427705f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can then train the model:",
   "id": "d18bc1c8146d8808"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T10:53:27.101032Z",
     "start_time": "2025-01-28T10:53:27.098124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = NeuralNet().to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "24e14f266b3e4895",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T10:53:40.141145Z",
     "start_time": "2025-01-28T10:53:27.106798Z"
    }
   },
   "cell_type": "code",
   "source": "train_model(model, train_loader, optimizer, criterion, num_epochs=200)",
   "id": "7b5ed45111d1aa10",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 0.6619\n",
      "Epoch [2/200], Loss: 0.6250\n",
      "Epoch [3/200], Loss: 0.6223\n",
      "Epoch [4/200], Loss: 0.6180\n",
      "Epoch [5/200], Loss: 0.6227\n",
      "Epoch [6/200], Loss: 0.6187\n",
      "Epoch [7/200], Loss: 0.6219\n",
      "Epoch [8/200], Loss: 0.6167\n",
      "Epoch [9/200], Loss: 0.6175\n",
      "Epoch [10/200], Loss: 0.6130\n",
      "Epoch [11/200], Loss: 0.6105\n",
      "Epoch [12/200], Loss: 0.6122\n",
      "Epoch [13/200], Loss: 0.6073\n",
      "Epoch [14/200], Loss: 0.6168\n",
      "Epoch [15/200], Loss: 0.6147\n",
      "Epoch [16/200], Loss: 0.6097\n",
      "Epoch [17/200], Loss: 0.6107\n",
      "Epoch [18/200], Loss: 0.6090\n",
      "Epoch [19/200], Loss: 0.6095\n",
      "Epoch [20/200], Loss: 0.6071\n",
      "Epoch [21/200], Loss: 0.6076\n",
      "Epoch [22/200], Loss: 0.6084\n",
      "Epoch [23/200], Loss: 0.6120\n",
      "Epoch [24/200], Loss: 0.6161\n",
      "Epoch [25/200], Loss: 0.6072\n",
      "Epoch [26/200], Loss: 0.6053\n",
      "Epoch [27/200], Loss: 0.6050\n",
      "Epoch [28/200], Loss: 0.6028\n",
      "Epoch [29/200], Loss: 0.6074\n",
      "Epoch [30/200], Loss: 0.6052\n",
      "Epoch [31/200], Loss: 0.6065\n",
      "Epoch [32/200], Loss: 0.6094\n",
      "Epoch [33/200], Loss: 0.6030\n",
      "Epoch [34/200], Loss: 0.6040\n",
      "Epoch [35/200], Loss: 0.6044\n",
      "Epoch [36/200], Loss: 0.6026\n",
      "Epoch [37/200], Loss: 0.6042\n",
      "Epoch [38/200], Loss: 0.6079\n",
      "Epoch [39/200], Loss: 0.6101\n",
      "Epoch [40/200], Loss: 0.6121\n",
      "Epoch [41/200], Loss: 0.6110\n",
      "Epoch [42/200], Loss: 0.6107\n",
      "Epoch [43/200], Loss: 0.6082\n",
      "Epoch [44/200], Loss: 0.6055\n",
      "Epoch [45/200], Loss: 0.6053\n",
      "Epoch [46/200], Loss: 0.6048\n",
      "Epoch [47/200], Loss: 0.6028\n",
      "Epoch [48/200], Loss: 0.6021\n",
      "Epoch [49/200], Loss: 0.6044\n",
      "Epoch [50/200], Loss: 0.5996\n",
      "Epoch [51/200], Loss: 0.6079\n",
      "Epoch [52/200], Loss: 0.6077\n",
      "Epoch [53/200], Loss: 0.5945\n",
      "Epoch [54/200], Loss: 0.5988\n",
      "Epoch [55/200], Loss: 0.5987\n",
      "Epoch [56/200], Loss: 0.6044\n",
      "Epoch [57/200], Loss: 0.6045\n",
      "Epoch [58/200], Loss: 0.6086\n",
      "Epoch [59/200], Loss: 0.5935\n",
      "Epoch [60/200], Loss: 0.6044\n",
      "Epoch [61/200], Loss: 0.5994\n",
      "Epoch [62/200], Loss: 0.6055\n",
      "Epoch [63/200], Loss: 0.6039\n",
      "Epoch [64/200], Loss: 0.6001\n",
      "Epoch [65/200], Loss: 0.6035\n",
      "Epoch [66/200], Loss: 0.6028\n",
      "Epoch [67/200], Loss: 0.5935\n",
      "Epoch [68/200], Loss: 0.6004\n",
      "Epoch [69/200], Loss: 0.6041\n",
      "Epoch [70/200], Loss: 0.5957\n",
      "Epoch [71/200], Loss: 0.6002\n",
      "Epoch [72/200], Loss: 0.5972\n",
      "Epoch [73/200], Loss: 0.5989\n",
      "Epoch [74/200], Loss: 0.6033\n",
      "Epoch [75/200], Loss: 0.6003\n",
      "Epoch [76/200], Loss: 0.5956\n",
      "Epoch [77/200], Loss: 0.6003\n",
      "Epoch [78/200], Loss: 0.5937\n",
      "Epoch [79/200], Loss: 0.6058\n",
      "Epoch [80/200], Loss: 0.5963\n",
      "Epoch [81/200], Loss: 0.6003\n",
      "Epoch [82/200], Loss: 0.5922\n",
      "Epoch [83/200], Loss: 0.5986\n",
      "Epoch [84/200], Loss: 0.5971\n",
      "Epoch [85/200], Loss: 0.6031\n",
      "Epoch [86/200], Loss: 0.5993\n",
      "Epoch [87/200], Loss: 0.5996\n",
      "Epoch [88/200], Loss: 0.6026\n",
      "Epoch [89/200], Loss: 0.5991\n",
      "Epoch [90/200], Loss: 0.5966\n",
      "Epoch [91/200], Loss: 0.5965\n",
      "Epoch [92/200], Loss: 0.5959\n",
      "Epoch [93/200], Loss: 0.6048\n",
      "Epoch [94/200], Loss: 0.5920\n",
      "Epoch [95/200], Loss: 0.5954\n",
      "Epoch [96/200], Loss: 0.5929\n",
      "Epoch [97/200], Loss: 0.6107\n",
      "Epoch [98/200], Loss: 0.5968\n",
      "Epoch [99/200], Loss: 0.5936\n",
      "Epoch [100/200], Loss: 0.5927\n",
      "Epoch [101/200], Loss: 0.5956\n",
      "Epoch [102/200], Loss: 0.5950\n",
      "Epoch [103/200], Loss: 0.5890\n",
      "Epoch [104/200], Loss: 0.6038\n",
      "Epoch [105/200], Loss: 0.5926\n",
      "Epoch [106/200], Loss: 0.5944\n",
      "Epoch [107/200], Loss: 0.6002\n",
      "Epoch [108/200], Loss: 0.5922\n",
      "Epoch [109/200], Loss: 0.5916\n",
      "Epoch [110/200], Loss: 0.6026\n",
      "Epoch [111/200], Loss: 0.5849\n",
      "Epoch [112/200], Loss: 0.5913\n",
      "Epoch [113/200], Loss: 0.5921\n",
      "Epoch [114/200], Loss: 0.5888\n",
      "Epoch [115/200], Loss: 0.5988\n",
      "Epoch [116/200], Loss: 0.5965\n",
      "Epoch [117/200], Loss: 0.5892\n",
      "Epoch [118/200], Loss: 0.5983\n",
      "Epoch [119/200], Loss: 0.5941\n",
      "Epoch [120/200], Loss: 0.5879\n",
      "Epoch [121/200], Loss: 0.5929\n",
      "Epoch [122/200], Loss: 0.5986\n",
      "Epoch [123/200], Loss: 0.5888\n",
      "Epoch [124/200], Loss: 0.5870\n",
      "Epoch [125/200], Loss: 0.5919\n",
      "Epoch [126/200], Loss: 0.5972\n",
      "Epoch [127/200], Loss: 0.5889\n",
      "Epoch [128/200], Loss: 0.5871\n",
      "Epoch [129/200], Loss: 0.5872\n",
      "Epoch [130/200], Loss: 0.5907\n",
      "Epoch [131/200], Loss: 0.5878\n",
      "Epoch [132/200], Loss: 0.5937\n",
      "Epoch [133/200], Loss: 0.5959\n",
      "Epoch [134/200], Loss: 0.5879\n",
      "Epoch [135/200], Loss: 0.5909\n",
      "Epoch [136/200], Loss: 0.5923\n",
      "Epoch [137/200], Loss: 0.5894\n",
      "Epoch [138/200], Loss: 0.5936\n",
      "Epoch [139/200], Loss: 0.5912\n",
      "Epoch [140/200], Loss: 0.5915\n",
      "Epoch [141/200], Loss: 0.5814\n",
      "Epoch [142/200], Loss: 0.5869\n",
      "Epoch [143/200], Loss: 0.5850\n",
      "Epoch [144/200], Loss: 0.5846\n",
      "Epoch [145/200], Loss: 0.5906\n",
      "Epoch [146/200], Loss: 0.5869\n",
      "Epoch [147/200], Loss: 0.5838\n",
      "Epoch [148/200], Loss: 0.5886\n",
      "Epoch [149/200], Loss: 0.5913\n",
      "Epoch [150/200], Loss: 0.5944\n",
      "Epoch [151/200], Loss: 0.5889\n",
      "Epoch [152/200], Loss: 0.5918\n",
      "Epoch [153/200], Loss: 0.5919\n",
      "Epoch [154/200], Loss: 0.5919\n",
      "Epoch [155/200], Loss: 0.5992\n",
      "Epoch [156/200], Loss: 0.5811\n",
      "Epoch [157/200], Loss: 0.5842\n",
      "Epoch [158/200], Loss: 0.5883\n",
      "Epoch [159/200], Loss: 0.5892\n",
      "Epoch [160/200], Loss: 0.5946\n",
      "Epoch [161/200], Loss: 0.5979\n",
      "Epoch [162/200], Loss: 0.5883\n",
      "Epoch [163/200], Loss: 0.5997\n",
      "Epoch [164/200], Loss: 0.5927\n",
      "Epoch [165/200], Loss: 0.5840\n",
      "Epoch [166/200], Loss: 0.5814\n",
      "Epoch [167/200], Loss: 0.5839\n",
      "Epoch [168/200], Loss: 0.5948\n",
      "Epoch [169/200], Loss: 0.5926\n",
      "Epoch [170/200], Loss: 0.5932\n",
      "Epoch [171/200], Loss: 0.5944\n",
      "Epoch [172/200], Loss: 0.5982\n",
      "Epoch [173/200], Loss: 0.5930\n",
      "Epoch [174/200], Loss: 0.5929\n",
      "Epoch [175/200], Loss: 0.5908\n",
      "Epoch [176/200], Loss: 0.5816\n",
      "Epoch [177/200], Loss: 0.5889\n",
      "Epoch [178/200], Loss: 0.5934\n",
      "Epoch [179/200], Loss: 0.5944\n",
      "Epoch [180/200], Loss: 0.5851\n",
      "Epoch [181/200], Loss: 0.5901\n",
      "Epoch [182/200], Loss: 0.5864\n",
      "Epoch [183/200], Loss: 0.5937\n",
      "Epoch [184/200], Loss: 0.5836\n",
      "Epoch [185/200], Loss: 0.5817\n",
      "Epoch [186/200], Loss: 0.5906\n",
      "Epoch [187/200], Loss: 0.5823\n",
      "Epoch [188/200], Loss: 0.5844\n",
      "Epoch [189/200], Loss: 0.5838\n",
      "Epoch [190/200], Loss: 0.5870\n",
      "Epoch [191/200], Loss: 0.5835\n",
      "Epoch [192/200], Loss: 0.5810\n",
      "Epoch [193/200], Loss: 0.5950\n",
      "Epoch [194/200], Loss: 0.5898\n",
      "Epoch [195/200], Loss: 0.5842\n",
      "Epoch [196/200], Loss: 0.5863\n",
      "Epoch [197/200], Loss: 0.5822\n",
      "Epoch [198/200], Loss: 0.5812\n",
      "Epoch [199/200], Loss: 0.5778\n",
      "Epoch [200/200], Loss: 0.5817\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T10:53:40.182109Z",
     "start_time": "2025-01-28T10:53:40.146359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Train evaluation:\")\n",
    "evaluate_model(model, train_loader)\n",
    "print(\"Test evaluation:\")\n",
    "evaluate_model(model, test_loader)"
   ],
   "id": "26a507b33862b5a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train evaluation:\n",
      "Accuracy: 69.90%\n",
      "Test evaluation:\n",
      "Accuracy: 68.19%\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
